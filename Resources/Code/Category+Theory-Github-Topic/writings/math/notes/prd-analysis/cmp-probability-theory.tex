\startcomponent *

\product  prd-analysis

\startchapter[title={Elementary ideas}]

	\startsection [title={\m{σ}-algebras}]
		\startitemize [1, nowhite, after]
			\item  \m{𝓘 ⊂ 𝓑 ⊂ \overline{𝓑} = 𝓛 ⊂ 2^ℝ}    % TODO: Add counterexamples
			\item  \m{\abs{𝓘} = \abs{𝓑} = \abs{ℝ} = ℵ_{1}, \ \abs{\overline{𝓑}} = \abs{2^ℝ} = ℵ_{2}}
		\stopitemize
	\stopsection

	\startsection [title={Examples}]
		\startitemize [1, nowhite, after]
			\item  \m{\brnd{x ↦ \frac{1}{\sqrt{x}} 𝟙_{[0, 1]}(x)} ∈ L^1 ∖ L^2}
			\item  \m{\brnd{x ↦ \frac{1}{x} 𝟙_{[1, ∞)}(x)} ∈ L^2 ∖ L^1}
		\stopitemize
	\stopsection

	\startsection [title={Measurability of \m{\inf, \sup, \liminf, \limsup} TODO}]
		Let \m{X_n} be a discrete-time stochastic process.
		Then \m{\bcrl{\inf X_t ≥ c}  =  ⋂_t \bcrl{X_t ≥ c}  }
	\stopsection

	\startsection [title={Method of substitution}]
		See Folland - Real Analysis Theorem (2.43).
	\stopsection


	\startsection [title={Bounds for Gaussian measure}]
		These ideas are from the following sources:
		\startitemize [1, joinedup]
			\item  \goto{John D. Cook --- Upper and lower bounds for the normal distribution function}[url(https://www.johndcook.com/blog/norm-dist-bounds)]
			\item  \goto{Dominic Yeo --- Gaussian tail bounds}[url(https://eventuallyalmosteverywhere.wordpress.com/2012/11/20/gaussian-tail-bounds-and-a-word-of-caution-about-clt)]
			\item  \goto{MSx post}[url(https://math.stackexchange.com/questions/518771)]
		\stopitemize

		Suppose \m{Z ∼ N(0, 1)}, and denote \m{G(z) = ℙ\bcrl{Z > z}} for \m{z ∈ [0, ∞)}. Then
		\startformula
			\frac{z}{z^2 + 1} < \sqrt{2π} e^{-\frac12 z^2} G(z) < \frac1z .
		\stopformula

		\startsubsubject [title={Upper bound}]
			\startformula
				G(z)
				=  ∫_z^∞ \frac{1}{\sqrt{2π}} e^{-\frac12 x^2} \d x
				≤  \frac{1}{\sqrt{2π} z} ∫_z^∞ e^{-\frac12 x^2} x \d x
				=  \frac{1}{\sqrt{2π} z} ∫_z^∞ e^{-\frac12 x^2} \d \brnd{\frac12 x^2}
				=  \frac{1}{\sqrt{2π} z} e^{-\frac12 z^2} .
			\stopformula
		\stopsubsubject

		\startsubsubject [title={Lower bound}]
			In this case, consider the function \m{h(z) = G(z) - \frac{1}{\sqrt{2π}} \frac{z}{z^2 + 1} e^{-\frac12 z^2}}. We shall show that \m{h > 0} on the domain. First we calculate \m{h'(z)}.
			\startformula  \startalign
				\NC  h'(z) =  \NC  G'(z) - \frac{1}{\sqrt{2π} (z^2 + 1)^2} \brnd{\brnd{e^{-\frac12 z^2} + z e^{-\frac12 z^2} (-z)} (z^2 + 1) - z e^{-\frac12 z^2} (2z)}  \NR
				\NC  =  \NC  \frac{e^{-\frac12 z^2}}{\sqrt{2π}} - \frac{e^{-\frac12 z^2}}{\sqrt{2π} (z^2 + 1)^2} \brnd{(1 - z^2)(z^2 + 1) - 2 z^2)}  \NR
				\NC  =  \NC  -\frac{e^{-\frac12 z^2}}{\sqrt{2π} (z^2 + 1)^2} .  \NR
			\stopalign  \stopformula

			Therefore, we have
			\startitemize [1, joinedup]
				\item  \m{h(0) = \frac12 > 0},
				\item  \m{h'(z) < 0} on the domain, and
				\item  \m{\lim_{z → ∞} h(z) = 0},
			\stopitemize
			which imply that \m{h > 0} on the domain.
		\stopsubsubject

	\stopsection

\stopchapter


\startchapter [title={Borel--Cantelli lemmas}]

	\starttheorem[title={Borel--Cantelli 1}]
		Let \m{(E_n) ⊂ ℱ} such that \m{∑ℙ(E_n) < ∞}. Then \m{ℙ\brnd{E_n \infinitelyoften} = 0}.
	\stoptheorem
	\startproof
		Since \m{∑ℙ(E_n) < ∞}, for any fixed \m{n ∈ ℕ}, we have
		\startformula
			ℙ\brnd{E_n \infinitelyoften}  =  ℙ\brnd{⋂_{n} ⋃_{m ≥ n} E_m}  ≤  ℙ\brnd{⋃_{m ≥ n} E_m}  ≤  ∑_{m ≥ n} ℙ(E_n)  →  0 .
		\stopformula
	\stopproof

	Counterexample of the converse of BC1: Take \m{((0, 1], λ)} as the probability space, and \m{E_n = \brnd{0, \frac{1}{n^2}}}. Then \m{ℙ\brnd{E_n \infinitelyoften} = 1}, but \m{∑ℙ(E_n) < ∞}.

	\starttheorem[title={Borel--Cantelli 2}]
		Let \m{(E_n) ⊂ ℱ} be (mutually) independent such that \m{∑ℙ(E_n) = ∞}. Then \m{ℙ\brnd{E_n \infinitelyoften} = 1}.
	\stoptheorem
	\startproof
		Since \m{ℙ\brnd{\brnd{E_n \infinitelyoften}^∁} = ℙ\brnd{E_n^∁ \eventually}}, it is equivalent to prove that \m{ℙ\brnd{E_n^∁ \eventually} = 0}. Using independence and the fact that \m{1 - x < e^{-x}}, for each fixed \m{n ∈ ℕ}, we have
		\startformula
			ℙ\brnd{⋂_{m = n}^N E_n^∁}  =  ∏_{m = n}^N ℙ\brnd{E_n^∁}  =  ∏_{m = n}^N \brnd{1 - ℙ(E_n)}  ≤  ∏_{m = n}^N e^{- ℙ(E_n)}  =  e^{- ∑_{m = n}^N ℙ(E_n)} .
		\stopformula
		Taking \m{N → ∞}, we get \m{ℙ\brnd{⋂_{m ≥ n} E_n^∁} → 0}. Therefore
		\startformula
			ℙ\brnd{E_n^∁ \eventually}  =  ℙ\brnd{⋃_n ⋂_{m ≥ n} E_n^∁}   ≤  ∑_n ℙ\brnd{⋂_{m = n}^N E_n^∁}  =  0 .
		\stopformula
	\stopproof

\stopchapter


\startchapter [title={Modes of convergence}]

	Study this part from \goto{Robert L Wolpert - Convergence in \m{ℝ^d} and in metric spaces}[url(https://www2.stat.duke.edu/courses/Fall18/sta711/lec/wk-07.pdf)].
	% ToDo: Correct this diagram.
	In this diagram, the top row represents \quote{\emph{point independent}} modes of convergence and the bottom row represents the \quote{\emph{point dependent}} modes of convergence.

	\blank[line]

	\starttikzcd [row sep=huge, column sep=huge]
		\text{unif}
				\arrow[r, middlegreen]
				\arrow[d, middlegreen]
			\NC  L^∞
				\arrow[r, middlegreen]
				\arrow[d, middlegreen]
			\NC  L^2
				\arrow[r, middlegreen]
				\arrow[dl, slategray, swap, "(X_n ⟂) ∧ (𝔼 X_n = 0) ∧ (∑ 𝕍 X_n < ∞)" description]
			\NC  L^1
				\arrow[r, middlegreen]
			\NC  ℙ
				\arrow[r, middlegreen]
				\arrow[dlll, bend left, magenta, "∀ \text{ subseq } ∃ \text{ subsubseq }" description]
				\arrow[l, bend right, darkblue, swap, "\text{u.i. ∨ unif bound}"]
			\NC  w^*
				\arrow[l, bend right, swap, darkorange, "const" description]
		\NR  \text{pt}
				\arrow[r, middlegreen]
			\NC  \text{a.s.}
				\arrow[urrr, middlegreen]
				\arrow[urr, dotted, red, near end, "LDCT?" description]  \NR
	\stoptikzcd

	Legend
	\startitemize [1, nowhite, after]
		\item  \color[middlegreen]{green: automatic implication}
		\item  any other color: depends on the mentioned condition
	\stopitemize

\stopchapter


\startchapter [title={Conditioning}, reference=sec:conditioning]

	\startsection[title={In \m{L^2}, conditional expectation is a projection}]
		Let \m{(Ω, ℱ, ℙ)} be a probability space, \m{𝓖 ⊆ ℱ} be a sub-σ-algebra, and \m{X ∈ L^2(ℱ)} be a random variable. Let \m{π_{L^2(𝓖)}} denote the projection operator onto \m{L^2(𝓖)}. We know that projection operators are self-adjoint. So \m{∀ E ∈ 𝓖},
		\startformula \startalign[n=3]
			\NC  ∫_E 𝔼(X ∣ 𝓖) \d ℙ \big|_𝓖  \NC =  ∫_E X \d ℙ  =  ∫ 𝟙_E X \d ℙ  =  \inn{𝟙_E, X}  \NC  [\text{definitions}]  \NR
			\NC  \NC =  \inn{π_{L^2(𝓖)} 𝟙_E, X}  \NC  [E ∈ 𝓖 ⟹ 𝟙_E ∈ L^2(𝓖)]  \NR
			\NC  \NC =  \inn{𝟙_E, π_{L^2(𝓖)}^* X}  =  \inn{𝟙_E, π_{L^2(𝓖)} X}  \NC  [\text{self-adjointness}]  \NR
			\NC  \NC =  ∫ 𝟙_E π_{L^2(𝓖)} X \d ℙ \big|_𝓖  =  ∫_E π_{L^2(𝓖)} X \d ℙ \big|_𝓖 .  \qquad  \NC  [\text{definitions}]  \NR
		\stopalign \stopformula
		Therefore, \m{𝔼(X ∣ 𝓖) = π_{L^2(𝓖)} X} a.s.

	\stopsection

	\startsection [title={Uncorrelated does not imply independence}]
		See the wikipedia entries on \goto{uncorrelated random variables}[url(https://en.wikipedia.org/wiki/Uncorrelated_random_variables)] and \goto{normally distributed and uncorrelated does not imply independent}[url(https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent)].
	\stopsection

	\startsection [title={\m{ϕ_{aX + bY} = ϕ_{aX} ϕ_{bY} \ ∀ (a, b) ∈ ℝ^2} implies independence}]
		See \goto{MathSx:1802289}[url(https://math.stackexchange.com/questions/1802289)].
	\stopsection

\stopchapter

\startchapter [title={Limiting behaviour of \m{\bar{X}_n}}]

	A great resource for this chapter is the Wikipedia article on \goto{central limit theorem}[url(https://en.wikipedia.org/wiki/Central_limit_theorem#Relation_to_the_law_of_large_numbers)].

	Let \m{(X_n)} be a sequence of independent and identically distributed random variables with mean \m{m} and distribution \m{μ}, and let \m{\bar{X}_n = \frac1n S_n = \frac1n ∑_{j = 1}^n X_j}.

	There are three scales for which we have three theorems, namely
	\startitemize[n, nowhite, after]
		\item  law of large numbers (\m{\bar{X}_n \xrightarrow{a.s.} m}),
		\item  law of the iterated logarithm (\m{\limsupm \frac{\bar{X}_n}{\sqrt{\frac{2 \log \log n}{n}}} = 1} a.s.), and
		\item  central limit theorem (\m{\sqrt{n}(\bar{X}_n - m) \xrightarrow{w^*} 𝓝(0, Σ)}).
	\stopitemize

	The idea is that we have an asymptotic expansion of \m{\bar{X}_n} given (in law) by
	\startformula
		\bar{X}_n ∼ m + \frac{1}{\sqrt{n}} 𝓝(0, Σ) , \quad \text{where \m{Σ} is the covariance operator.}
	\stopformula
	The convergence to \m{m} is given by the law of large numbers, and the convergence to \m{\frac{1}{\sqrt{n}} 𝓝(0, Σ)} is given by the central limit theorem. As \m{n → ∞}, the dependence on \m{𝓝(0, Σ)} goes to zero, so this is consistent with the law of large numbers.

	\startsubsubsubject [title={Central limit theorem: how to remember in \m{1}-dim}]
		\startformula
			\frac{\bar{X}_n - 𝔼 \bar{X}_n}{\sqrt{𝕍 \bar{X}_n}}
			=  \frac{\bar{X}_n - m}{\frac{σ}{\sqrt{n}}}
			\xrightarrow[small]{w^*}  𝓝(0, 1)
		\stopformula
	\stopsubsubsubject

\stopchapter

\stopcomponent
