\startcomponent *

\product  prd-analysis


\startchapter [title={Brownian motion}]

	In the following, let \m{i ∈ [n]} for \m{n ∈ ℕ}, and
	\startitemize [5, joinedup]
		\item  \m{Δ_i t = t_i - t_{i-1}} with \m{Δ_1 t = t_1},
		\item  \m{Δ_i x = x_i - x_{i-1}}, with  and \m{Δ_1 x = x_1},
		\item  \m{Δ_i X = X_{t_i} - X_{t_{i-1}}}, with  and \m{Δ_1 X = X_{t_1}}.
	\stopitemize

	\starttheorem
		The following are equivalent
		\startitemize [m, joineup]
			\item[bm-increment]  \m{X_t} is a stochastic process having independent Gaussian increments.
			\item[bm-marginal]  \m{X_t} is a stochastic process with marginal distributions given by
				\startformula
					μ_{t_1, …, t_n}(A) = \frac{1}{\sqrt{(2π)^n ∏ Δ_i t}} ∫_A \exp \brnd{-\frac12 ∑ \frac{(Δ_i x)^2}{Δ_i t}} ∏ \d x_i
				\stopformula
				for any \m{0 < t_1 < ⋯ < t_n} and \m{n ∈ ℕ}.
			\item[bm-Fourier]  \m{X_t} is a stochastic process such that for any \m{0 < t_1 < ⋯ < t_n} and \m{λ_1, ⋯, λ_n ∈ ℝ},
				\startformula
					𝔼 \exp \brnd{𝚤 ∑ λ_i Δ_i X} = \exp\brnd{-\frac12 ∑ λ_i^2 Δ_i t} .
				\stopformula
		\stopitemize
	\stoptheorem

	\startproof
		Firstly, let \m{Π = \bcrl{(-∞, c_1] × ⋯ × (-∞, c_n]: c_i ∈ ℝ, n ∈ ℕ}}, and note that \m{Π} is a π-system that generates the Borel sigma algebra. So it is enough to show the results for an arbitrary set in \m{Π}. Moreover, the idea of the proof is the same for \m{n ≥ 2}, so we shall consider \m{n = 2}. Therefore, \in[bm-marginal] reduces to
		\startformula
			μ_{t_1, t_2}((-∞, c_1] × (-∞, c_2]) = \frac{1}{\sqrt{(2π)^2 t_1 (t_2 - t_1)}} ∫_{-∞}^{c_1} ∫_{-∞}^{c_2} e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}}  \d x_2 \d x_1 ,
		\stopformula
		and \in[bm-Fourier] reduces to
		\startformula
			𝔼 e^{𝚤 (λ_1 X_{t_1} + λ_2 (X_{t_2} - X_{t_1}))} = e^{-\frac12 (λ_1^2 t_1 + λ_2^2 (t_2 - t_1))} .
		\stopformula
		Now we start with the proof.
		\startitemize [i]
			\item  \in[bm-increment] ⟹ \in[bm-marginal]
				\startformula  \startalign[align={left, left}]
					\NC  μ_{t_1, t_2}((-∞, c_1] × (-∞, c_2])  \NR
					\NC  =  ℙ \bcrl{X_{t_1} ≤ c_1, X_{t_2} ≤ c_2}  \NR
					\NC  =  ∫_{-∞}^{c_1} ∫_{-∞}^{c_2}  ℙ \bcrl{X_{t_2} ∈ \d x_2 ∣ X_{t_1} = x_1}  ℙ \bcrl{X_{t_1} ∈ \d x_1}  \NR
					\NC  =  ∫_{-∞}^{c_1} ∫_{-∞}^{c_2}  ℙ \bcrl{X_{t_2} - X_{t_1} ∈ \d x_2 - x_1 ∣ X_{t_1} = x_1}  ℙ \bcrl{X_{t_1} ∈ \d x_1}  \NR
					\NC  =  ∫_{-∞}^{c_1} ∫_{-∞}^{c_2}  ℙ \bcrl{X_{t_2} - X_{t_1} ∈ \d x_2 - x_1}  ℙ \bcrl{X_{t_1} ∈ \d x_1}  \qquad  \NC  \comment{[\text{independent incr}]}  \NR
					\NC  =  ∫_{-∞}^{c_1} ∫_{-∞}^{c_2}  \frac{1}{\sqrt{(2π) (t_2 - t_1)}} e^{-\frac12 \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}  \d x_2  \frac{1}{\sqrt{(2π) t_1}} e^{-\frac12 \frac{x_1^2}{t_1}}  \d x_1  \qquad  \NC  \comment{[\text{Gaussian incr}]}  \NR
					\NC  =  \frac{1}{\sqrt{(2π)^2 t_1 (t_2 - t_1)}} ∫_{-∞}^{c_1} ∫_{-∞}^{c_2} e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}} \d x_2 \d x_1 .  \NR
				\stopalign  \stopformula

			\item  \in[bm-marginal] ⟹ \in[bm-Fourier]
				First, we recall the characteristic function of \m{X ∼ 𝒩(0, t)} as \m{𝔼e^{𝚤 λ X} = e^{-\frac12 λ^2 t}}.
				Secondly, note that
				\startformula
					𝔼 e^{𝚤 (λ_1 X_{t_1} + λ_2 (X_{t_2} - X_{t_1}))}
					=  ∫_ℝ ∫_ℝ  e^{𝚤 (λ_1 x_1 + λ_2 (x_2 - x_1))}  \frac{e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}}}{\sqrt{(2π)^2 t_1 (t_2 - t_1)}} \d x_2 \d x_1 .
				\stopformula
				Now, using the change of variables \m{y_1 = x_1, y_2 = x_2 - x_1}, we have \m{\d y_1 = \d x_1} and \m{\d y_2 = \d x_2 - \d x_1}, so \m{\d x_2 \d x_1 = (\d y_2 + \d y_1) \d y_1 = \d y_2 \d y_1}. Therefore,
				\startformula  \startalign[align={left, left}]
					\NC  𝔼 e^{𝚤 (λ_1 X_{t_1} + λ_2 (X_{t_2} - X_{t_1}))}  \NR
					\NC  =  ∫_ℝ ∫_ℝ  e^{𝚤 (λ_1 y_1 + λ_2 y_2)}  \frac{e^{-\frac12 \brnd{\frac{y_1^2}{t_1} + \frac{y_2^2}{t_2 - t_1}}}}{\sqrt{(2π)^2 t_1 (t_2 - t_1)}}  \d y_2 \d y_1  \NR
					\NC  =  \brnd{∫_ℝ  e^{𝚤 λ_1 y_1}  \frac{e^{-\frac12 \frac{y_1^2}{   t_1   }}}{\sqrt{(2π)     t_1    }}  \d y_1}
					        \brnd{∫_ℝ  e^{𝚤 λ_2 y_2}  \frac{e^{-\frac12 \frac{y_2^2}{t_2 - t_1}}}{\sqrt{(2π) (t_2 - t_1)}}  \d y_2}  \NR
					\NC  =  \brnd{e^{-\frac12 λ_1^2 t_1}} \brnd{e^{-\frac12 λ_2^2 (t_2 - t_1)}}  \quad
					     =  \quad  e^{-\frac12 (λ_1^2 t_1 + λ_2^2 (t_2 - t_1))}  .  \NR
				\stopalign  \stopformula

			\item  \in[bm-Fourier] ⟹ \in[bm-increment]

		\stopitemize
	\stopproof
\stopchapter


\startchapter [title={Classification of stochastic processes}]

	This is well written in Cosma Rohilla Shalizi - Almost None of the Theory of Stochastic (2010), Chapter 1. Let \m{X} be a stochastic process given by
	\startformula \startalign[n=3, align={right,right,left}]
		\NC  X :  \NC  𝕋  ×   Ω  \NC  →  Ξ  \NR
		\NC       \NC  \qquad ℱ  \NC  →  𝓧  \NR
		\NC       \NC  (t,    ω)  \NC  ↦  X(t, ω) .  \NR
	\stopalign \stopformula

	The spaces are as follows.

	\dscr{\m{𝕋}}  The \emph{index set}. Can be finite, discrete (countable) or continuous (uncountable). Can be one-sided, two-sided, spatially distributed, or sets.

	\dscr{\m{(Ξ, 𝓧)}}  The \emph{state space}. Requirements: measurable. Can be finite, discrete or continuous.

	\dscr{\m{(Ω, ℱ, ℙ)}}  The \emph{probability space}.

	\startitemize [1, nowhite, after]
		\item  If \m{𝕋 = \bcrl{1}, Ξ = ℝ}, then \m{X} is a \emph{random variable}.
		\item  If \m{𝕋 = \bcrl{1, …, n}, Ξ = ℝ}, then \m{X} is a \emph{random vector}.
		\item  If \m{𝕋 = \bcrl{1}, Ξ = ℝ^d}, then \m{X} is a \emph{random vector}.
		\item  If \m{𝕋 = ℕ, Ξ = ℝ}, then \m{X} is a \emph{one-sided random sequence} or \emph{one-sided discrete-time stochastic process}.
		\item  If \m{𝕋 = ℤ, Ξ = ℝ}, then \m{X} is a \emph{two-sided random variable} or \emph{two-sided discrete-time stochastic process}.
		\item  If \m{𝕋 = ℤ^d, Ξ = ℝ}, then \m{X} is a \emph{spatial random variable}.
		\item  If \m{𝕋 = ℝ, Ξ = ℝ}, then \m{X} is a \emph{continuous-time random variable}.
		\item  If \m{𝕋 = ℬ, Ξ = [0, ∞]}, then \m{X} is a \emph{random set function on the reals}.
		\item  If \m{𝕋 = ℬ × ℕ, Ξ = [0, ∞]}, then \m{X} is a \emph{one-sided random sequence of set function on the reals}.
		\item  \emph{Emperical measures.}  Let \m{(Z_n)} be an i.i.d. random sequence and define \m{\hat{ℙ}_n : ℬ × Ω  \NC  →  𝓟: (B, ω)  ↦  \hat{ℙ}_n(B, ω)  =  \frac{1}{n} ∑_{j = 1}^n 𝟙_B(Z_j(ω))}. Then \m{\hat{ℙ}_n} is a \emph{one-sided random sequence of set function on the reals}, which are in fact \emph{probability measures}. \comment{[\m{𝓟} is the space of probability measures on \m{ℝ}.]}
		\item  If \m{𝕋 = ℬ^d, Ξ = [0, ∞]}, then \m{X} is the class of set functions on \m{ℝ^d}. Let \m{𝓜} be the subclass of measures. Then a random set function with realizations in \m{𝓜} is called a \emph{random measure}.
		\item  If \m{𝕋 = ℬ^d, \abs{Ξ} < ∞}, then \m{X} is a \emph{point process}.
		\item  If \m{𝕋 = [0, ∞), Ξ = ℝ^d < ∞}. A \m{Ξ}-valued random process on \m{𝕋} with paths in \m{C(𝕋)} is a \emph{continuous random process}. E.g. Wiener process.
	\stopitemize

\stopchapter


\startchapter [title={Martingales}, reference=sub:martingales]

	\startsection [title={New martingales from old}]

		A stochastic process \m{A = (A_n)} is called adapted if \m{∀ n ∈ ℕ, A_n ∈ L^0(ℱ_n)}. Let \m{M = (M_n)} be a martingale. Then process \m{\tilde{M} = (\tilde{M}_n)} defined by \m{(A ⋅ M)_n = \tilde{M}_n = ∑_{j = 0}^{n - 1} A_j ΔM_j}, where \m{ΔM_j = M_{j + 1} - M_j}, is called the \emph{martingale transform} of \m{M} by \m{A}.

		\starttheorem [title={martingale transform theorem}]
			\m{\tilde{M}} is a martingale.
		\stoptheorem
		\startproof
			\m{𝔼(Δ \tilde{M}_n ∣ ℱ_n)  =  𝔼(A_n ΔM_n ∣ ℱ_n)  =  A_n 𝔼(ΔM_n ∣ ℱ_n)  =  0}.
		\stopproof

		Now, let \m{X_n} be a stochastic process and \m{τ} be a stopping time. Define the stopped process \m{X_τ = ∑_{j = 0}^∞ 𝟙_{\bcrl{τ = j}} X_j} when \m{ℙ(τ < ∞) = 1}.

		\starttheorem [title={stopping time theorem}]
			Let \m{(M_n)} be a martingale with respect to \m{(ℱ_n)}. Then \m{(M_{n ∧ τ})} is also a martingale with respect to \m{(ℱ_n)}.
		\stoptheorem
		\startproof
			Without loss of generality, assume \m{M_0 = 0}, otherwise we can translate by \m{M_0} as \m{\tilde{M_n} = M_n - M_0}. Now, the \emph{stake process} \m{A_n = 𝟙_{\bcrl{τ > n}} = 1 - 𝟙_{\bcrl{τ ⩽ n}}} is adapted to \m{(ℱ_n)} and is bounded by \m{n}. Now,
			\startformula \startalign
				\NC  (A ⋅ M)_n  \NC =  ∑_{j = 0}^{n - 1} A_j ΔM_j  \NR
				\NC  \NC =  ∑_{j = 0}^{n - 1} ΔM_j - ∑_{j = 0}^{n - 1} 𝟙_{\bcrl{τ ⩽ j}} \brnd{M_{j + 1} - M_j}  \NR
				\NC  \NC =  M_n - M_0 - M_n 𝟙_{\bcrl{τ ⩽ n}} + ∑_{j = 0}^{n - 1} \brnd{𝟙_{\bcrl{τ ⩽ j}} M_j - 𝟙_{\bcrl{τ ⩽ {j - 1}}} M_j}  \NR
				\NC  \NC =  M_n 𝟙_{\bcrl{τ > n}} + ∑_{j = 0}^{n - 1} M_j 𝟙_{\bcrl{τ = j}}   \NR
				\NC  \NC =  M_n 𝟙_{\bcrl{τ > n}} + ∑_{j = 0}^{n - 1} M_τ 𝟙_{\bcrl{τ = j}}   \NR
				\NC  \NC =  M_n 𝟙_{\bcrl{τ > n}} + M_τ ∑_{j = 0}^{n - 1} 𝟙_{\bcrl{τ = j}}   \NR
				\NC  \NC =  M_n 𝟙_{\bcrl{τ > n}} + M_τ 𝟙_{\bcrl{τ ⩽ n}}   \NR
				\NC  \NC =  M_{n ∧ τ} .  \NR
			\stopalign \stopformula
			Therefore, \m{(M_{n ∧ τ})} is a martingale transform of \m{(M_n)}. Since \m{(A_n)} is bounded and adapted, by the martingale transform theorem, \m{(M_{n ∧ τ})} is a martingale.
		\stopproof

	\stopsection

\stopchapter


\startchapter [title={Markov chains}]

	\startsection [title={First Step Analysis}]
		First-step analysis is a general strategy for solving many Markov chain problems by conditioning on the first step of the Markov chain.

		We understand this from the first example of \cite[Steele2001]. We will derive a recursive relationship of the probability of a gambler winning before he goes bankrupt. The setting is as follows.

		A gambler starts with a principal of \m{0}, and he can borrow a maximum of \m{b}. He stops playing if his net value is \m{a} at any point of time. At each instant \m{i}, his wealth \m{S_i} either increases or decreases by one amount depending on the output of the Bernoulli random variable \m{X_i} with \quote{up} probability \m{p}. This gives rise to the finite state space \m{𝒮 = \bcrl{-b, -b + 1, …, a - 1, a}}. Note that \m{(S_n)} is a time-homogeneous Markov chain on \m{𝒮} with transition probabilities as follows:
		\startitemize [n, joinedup]
			\item  \m{P_{-b, j} = P_{a, j} = 0} (absorbing barriers),
			\item  \m{P_{i, i + 1} = p} and \m{P_{i, i - 1} = q} with \m{q = 1 - p}, and
			\item  \m{P_{i, j} = 0} in all other cases.
		\stopitemize

		Let \m{τ} be the first exit time, and \m{f(k) = ℙ\bcrl{S_τ = A ∣ S_0 = k} = ℙ_{\bcrl{S_0 = k}} \bcrl{S_τ = A}} for \m{k ∈ 𝒮}. Our goal in this setting is to obtain a recursive relation for \m{f}. Now,
		\startformula
			\bcrl{S_τ = A}  =  \bcrl{S_τ = A} ∩ Ω  =  \bcrl{S_τ = A} ∩ ⨆_{l ∈ 𝒮} \bcrl{S_1 = l}  =  ⨆_{l ∈ 𝒮} \bcrl{S_τ = A} ∩ \bcrl{S_1 = l} .
		\stopformula
		Finally,
		\startformula \startalign[n=3]
			\NC  f(k)  \NC =  ℙ_{\bcrl{S_0 = k}} \bcrl{S_τ = A}  \NR
			\NC  \NC =  ∑_{l ∈ 𝒮} ℙ_{\bcrl{S_0 = k}} \brnd{\bcrl{S_τ = A} ∩ \bcrl{S_1 = l}}  \NR
			\NC  \NC =  ∑_{l ∈ 𝒮} ℙ_{\bcrl{S_0 = k}} \bcrl{S_τ = A ∣ S_1 = l} ℙ_{\bcrl{S_0 = k}} \bcrl{S_1 = l}  \NR
			\NC  \NC =  ∑_{l ∈ 𝒮} ℙ_{\bcrl{S_1 = l}} \bcrl{S_τ = A} P_{k, l}  \qquad  \NC  [\text{Markov property}]  \NR
			\NC  \NC =  ∑_{l ∈ 𝒮} P_{k, l} f(l)  .  \NC  [\text{time-homogenity}]  \NR
		\stopalign \stopformula

		Since \m{P_{k, l} = 0} for all \m{l} except for \m{k ± 1}, we get
		\startformula
			f(k) = f(k + 1) p + f(k - 1) q .
		\stopformula

	\stopsection
\stopchapter


\startchapter[title={Markov processes}, reference=sub:Markov]

	\startsubject[title={Equivalent definitions}]
		Let \m{s ∈ [0, t]}. Then \m{X_⋅} is a Markov process if any of the following are true:
		\startitemize[1, nowhite, after]
			\item  \m{∀ E ∈ ℱ, ℙ(X_t ∈ E ∣ ℱ_s) = ℙ(X_t ∈ E ∣ X_s)}, or
			\item  \m{∀ E ∈ ℱ, ∀ f ∈ L^0 ∩ ℬ, 𝔼(f(X_t) ∣ ℱ_s) = 𝔼(f(X_t) ∣ X_s)}.
		\stopitemize
	\stopsubject

	\startsubject[title={Martingale vs Markov}]
		See \goto{djalil.chafai.net}[url(http://djalil.chafai.net/blog/2012/01/20/martingales-which-are-not-markov-chains/)] and \goto{MathSx:763645}[url(https://math.stackexchange.com/questions/763645)].
	\stopsubject
\stopchapter


\startchapter [title={Itô calculus}, reference=sub:Itô]

	\startremark [title={Wiener integral}]
		The term \emph{Wiener integral} may be used to refer to either of the following two unrelated concepts
		\startitemize [i, joinedup]
			\item  Lebesgue integral w.r.t. the Wiener measure
			\item  Stochastic integral when the integrand is deterministic
		\stopitemize
	\stopremark


	\startremark [title={Notations}]
		In what follows, \m{T = [0, ∞)}, \m{𝒜} means adapted, \m{ℬ} means bounded, \m{𝒞} means continuous, and \m{\norm{⋅}} denotes the \m{L^2}-norm.

		Let \m{(Ω, ℱ, 𝔽 = (ℱ_t), ℙ)} be a filtered probability space, \m{W:T × Ω → ℂ} be a \m{𝔽}-adapted Wiener martingale, and \m{X: T × Ω → ℂ} be a stochastic process.
	\stopremark


	\startsection [title={Definition of the Itô integral}]

		\startsubsection [title={Step 1: \m{X ∈ 𝒜 ∩ 𝒮} a.s.}]

			Let \m{X(t, ω) = ∑_{j ≥ 0} ξ_j(ω) 𝟙_{[t_j, t_{j+1})}(t)}, where \m{ξ_j ∈ L^0(ℱ_{t_j})}.

		\stopsubsection

		\startsubsection [title={Step 2: \m{X ∈ 𝒜 ∩ ℬ ∩ 𝒞} a.s.}]

			Define \m{X_n(t, ω) = X\brnd{\frac{\floor{nt}}{n}, ω}, \ n ∈ ℕ}. Note that \m{∀n, X_n ∈ 𝒜 ∩ 𝒮}, and since \m{X ∈ 𝒞}, \m{\abs{X_n(t, ω) - X(t, ω)} → 0} (pointwise convergence) \m{(t, ω)}-a.s. Then \m{∀ ε > 0}, there exists a sufficiently large \m{n ∈ ℕ} such that \m{\abs{X_n(t, ω) - X(t, ω)} < ε < ∞} (bounded) \m{(t, ω)}-a.s, so \m{\abs{X_n(t, ω) - X(t, ω)}^2 < ε^2 < ∞} \m{(t, ω)}-a.s. Therefore, by the bounded convergence theorem, \m{\norm{X_n - X} → 0}.

			Therefore, \m{(X_n)} is Cauchy in \m{L^2(T × Ω)}, that is, \m{\norm{X_n - X_m} → 0}. Now, by linearity and Itô isometry for the Itô integral for simple processes, \m{\norm{ℐ(X_n) - ℐ(X_m)} = \norm{ℐ(X_n - X_m)} = \norm{X_n - X_m} → 0}. Therefore, for \m{t ∈ T} fixed, \m{(ℐ(X_n))} is Cauchy in \m{L^2(Ω)}. Since \m{L^2(Ω)} is complete, the sequence converges. Denote the limit by \m{ℐ(X)}, that is, \m{\norm{ℐ(X_n) - ℐ(X)} → 0}.

		\stopsubsection

		\startsubsection [title={Step 3: \m{X ∈ 𝒜 ∩ ℬ ∩ L^0(T × Ω)}}]

		\stopsubsection

		\startsubsection [title={Step 4: \m{X ∈ 𝒜 ∩ L^2(T × Ω)}}]

		\stopsubsection

		\startsubsection [title={Step 5: \m{X ∈ 𝒜 ∩ \bcrl{X ∈ ℂ^{T × Ω} : ∀ t ≥ 0, ∫_0^t X(s, ⋅) \d s < ∞} a.s.}}]

		\stopsubsection
	\stopsection


	\startsection [title={Properties of the Itô integral}]

		In what follows, assume the following. Let \m{X, Y ∈ 𝒜 ∩ L^2(T × Ω); (X_n), (Y_n) ⊂ 𝒜 ∩ 𝒮} such that \m{\norm{X_n - X} → 0} and \m{\norm{Y_n - Y} → 0}. Let \m{z ∈ ℂ}.

		\startsubsection [title={Linearity: \m{\norm{z ℐ(X) + ℐ(Y) - ℐ(zX+Y)} = 0}}]
			First, note that \m{\norm{(z X_n + Y_n) - (z X + Y)} ≤ \abs{z} \norm{X_n - X} + \norm{Y_n - Y} → 0}. Now, by the linearity of the integral \m{ℐ:𝒜 ∩ 𝒮 → L^2(Ω)}, we have
			\startformula \startalign[align={left}]
				\NC  \norm{z ℐ(X) + ℐ(Y) - ℐ(z X + Y)}  \NR
				\NC  =  \norm{z ℐ(X) + ℐ(Y) - z ℐ(X_n) - ℐ(Y_n) + ℐ(z X_n + Y_n) - ℐ(z X + Y)}  \NR
				\NC  ≤  \abs{z} \norm{ℐ(X) - ℐ(X_n)} + \norm{ℐ(Y) - ℐ(Y_n)} + \norm{ℐ(z X_n + Y_n) - ℐ(z X + Y)}  →  0 .
			\stopalign \stopformula
		\stopsubsection

		\startsubsection [title={Itô isometry: \m{\norm{ℐ(X)} = \norm{X}}}]
			Using the isometry of the integral \m{ℐ:𝒜 ∩ 𝒮 → L^2(Ω)}, we have
			\startformula \startalign[align={left,left}]
				\NC  \norm{ℐ(X)}  \NC  ≤  \norm{ℐ(X) - ℐ(X_n)} + \norm{ℐ(X_n)}  \NR
				\NC  \NC  =  \norm{ℐ(X) - ℐ(X_n)} + \norm{X_n}  \NR
				\NC  \NC  =  \norm{ℐ(X) - ℐ(X_n)} + \norm{X_n - X} + \norm{X}  →  \norm{X} .
			\stopalign \stopformula
			Note that the \quote{Itô isometry} is actually a unitary transformation.
		\stopsubsection

		\startsubsection [title={Martingale property: \m{𝔼\brnd{ℐ_t(X) ∣ ℱ_s} = ℐ_s(X)} a.s.}]
			The martingale property of the integral \m{ℐ:𝒜 ∩ 𝒮 → L^2(Ω)} gives \m{𝔼\brnd{ℐ_t(X_n) - ℐ_s(X_n) ∣ ℱ_s} = 0}. Using this and the unitariness of the Itô isometry, we get
			\startformula \startalign[align={left,left}]
				\NC  \norm{𝔼{\brnd{ℐ_t(X) - ℐ_s(X) ∣ ℱ_s}}}^2  \NR
				\NC  =  𝔼\abs{𝔼{\brnd{ℐ_t(X) - ℐ_t(X_n) + ℐ_s(X_n) - ℐ_s(X) ∣ ℱ_s}} + 𝔼{\brnd{ℐ_t(X_n) - ℐ_s(X_n) ∣ ℱ_s}}}^2  \NR
				\NC  =  𝔼\abs{𝔼{\brnd{ℐ_t(X) - ℐ_t(X_n) + ℐ_s(X_n) - ℐ_s(X) ∣ ℱ_s}} + 0}^2  \NR
				\NC  ≤  𝔼𝔼\brnd{\abs{ℐ_t(X) - ℐ_t(X_n) + ℐ_s(X_n) - ℐ_s(X)}^2 ∣ ℱ_s}  \NR
				\NC  =  𝔼\abs{ℐ_t(X) - ℐ_t(X_n) + ℐ_s(X_n) - ℐ_s(X)}^2  \NR
				\NC  =  \norm{ℐ_t(X - X_n) + ℐ_s(X_n - X)}^2  \NR
				\NC  ≤  2 \brnd{\norm{ℐ_t(X - X_n)}^2 + \norm{ℐ_s(X_n - X)}^2}  \qquad  \bsqr{\norm{a + b}^2 ≤ 2 \brnd{\norm{a}^2 + \norm{b}^2}}  \NR
				\NC  =  2 \brnd{\norm{X - X_n}^2 + \norm{X_n - X}^2}  =  4 \norm{X_n - X}^2  →  0 .
			\stopalign \stopformula
		\stopsubsection

	\stopsection

	% In what follows, \m{T = [0, t], \ t ∈ [0, ∞)}, \m{X: T × Ω → ℝ}, and \m{𝒮} represent the class of simple processes.

	% \startsection [title={Approximation of \m{𝒮 ∋ X_n → X ∈ L^2(T × Ω)}}]
	% 	\startsubsection [title={Step 1: \m{𝒮 ∋ X_n → X ∈ C ∩ B}}]
	% 		Suppose \m{X} is a progressive, adapted, \emph{bounded on \m{I}} \m{t}-a.s., and has \emph{continuous sample paths} \m{t}-a.s. Then there exist a sequence of bounded simple processes \m{(X_n)} such that \m{X_n → X} in \m{L^2(T × Ω)}.

	% 		Proof.

	% 		Let \m{(Δ_n)} be a sequence of progressively finer partitions of \m{I} such that \m{Δ_n = \bcrl{t_0, t_1 …, t_n]; t_0 < t_1 < … < t_n}, and \m{\norm{Δ_n] → 0} as \m{n → ∞}. Define the sequence of simple processes \m{X_n(t, ω) = ∑_{j = 0}^{n - 1} X(t_j, ω) 𝟙_{\intco[t_j, t_{j + 1}]}(t)}. Then, we claim that \m{X_n → X} in \m{L^2(T × Ω)}.

	% 		Firstly fix \m{ω ∈ Ω}. By the \emph{continuity of paths} of \m{X}, \m{∀ ϵ > 0, ∃ δ > 0} such that whenever \m{\abs{s - t] < δ}, we have \m{\abs{X(s, ω) - X(t, ω)] < ϵ}. Therefore, for \m{\norm{Δ_n] < δ}, we have
	% 		\startformula
	% 			\abs{X_n(t, ω) - X(t, ω)]
	% 				≤ ∑_{j = 0}^{n - 1} \abs{X(t_j, ω) - X(t, ω)] 𝟙_{\intco[t_j, t_{j + 1}]}(t)
	% 				< ∑_{j = 0}^{n - 1} ϵ 𝟙_{\intco[t_j, t_{j + 1}]}(t)
	% 				= ϵ .
	% 		\stopformula
	% 		Therefore, \m{∀ t ∈ I}, \m{\abs{X_n(t, ω) - X(t, ω)] → 0}, and so \m{\abs{X_n(t, ω) - X(t, ω)]^2 → 0}. Moreover, since the sample path \m{X(⋅, ω)} is \emph{bounded}, we have \m{\abs{X_n(t, ω) - X(t, ω)]^2 ⩽ 4 \norm{X(⋅, ω)]_{L^∞(I)}^2 < ∞}. Therefore by the bounded convergence theorem, \m{∫_0^1 \abs{X_n(t, ω) - X(t, ω)]^2 \d t → 0}.

	% 		Now varying \m{ω} and taking expectation, we get \m{𝔼\brnd{∫_0^1 {\abs{X_n(t, ω) - X(t, ω)]}^2 \d t] → 0}. In other words, \m{\norm{X_n - X]_{L^2(T × Ω)} → 0}.
	% 	\stopsubsection
	% 	\startsubsection [title={Step 2: \m{C ∩ B ∋ X_n → X ∈ B}}]

	% 	\stopsubsection
	% 	\startsubsection [title={Step 3: \m{B ∋ X_n → X ∈ L^2(T × Ω)}}]

	% 	\stopsubsection
	% \stopsection

	\startsection[title={Itô formula for multidimensional processes}]

		Recall the discussion of Taylor series for multivariate functionals in section \in[sec:taylor].

		This part is from \cite[SundarKallianpur2014], § 5.3, 5.4 and 5.6.

		A (local, continuous) semimartingale is a process \m{X_t} that can be written as \m{X_t = X_0 + M_t + A_t}, where
		\startitemize [n, joinedup, nowhite, after]
			\item  \m{M_t} is a mean-zero (local, continuous) martingale, and
			\item  \m{A_t} is an right-continuous adapted process of locally bounded variation.
		\stopitemize
		This is equivalently represented in the differential form as \m{\d X_t = \d M_t + \d A_t}.

		Let \m{X_t} be a \m{d}-dimensional semimartingale, and let \m{Y_t = f(X_t)}, where \m{f ∈ C^2(ℝ)}. Then
		\startformula
			\d Y_t
			=  \d f(X_t)
			=  f'(X_t) \d A_t
			 + f'(X_t) \d M_t
			 + \frac12 f''(X_t) \d \inn{M_t, M_t},
		\stopformula
		where we use the rule \m{\d \inn{B^{(j)}, B^{(j)}}_t = \brnd{\d B_t^{(j)}}^2 = \d t}, everything else being 0.

		Alternatively, as from \cite[Kuo2006], we have the following.

		Let \m{X_t= (X_t^{(1)}, \dots, X_t^{(d)})} be a \m{d}-dimensional process and \m{f(t, x)} be a functional of \m{(t, X_t)}. Then the Itô formula becomes
		\startformula
			\d f(t, X_t)  =  \brnd{\inn{\d t, \D_t} f}(t, X_t)
				+ \brnd{\inn{\d x, \D_x} f}(t, X_t)
				+ \frac12 \brnd{\inn{\d x, \D_x}^2 f}(t, X_t) ,
		\stopformula
		or in short, \m{\d f  =  \brnd{\inn{\d t, \D_t}  +  \inn{\d x, \D_x}  +  \frac12 \inn{\d x, \D_x}^2} f}.

	\stopsection

	\startsection[title={Adapted and instantly independent implies deterministic}]

		Let \m{X_⋅} be both adapted and instantly independent. Then for any fixed \m{t}, we have \m{X_t = 𝔼(X_t ∣ ℱ_t) = 𝔼 X_t}. Therefore, \m{X_t} is constant w.r.t. \m{ω} for each \m{t}. Therefore \m{X_⋅} must be deterministic.

	\stopsection

\stopchapter


\startchapter [title={Examples}]

	\startsection [title={Find \m{𝔼\brnd{∫_0^1 B_t^2 \d t}^2}.}]

		By Fubini's theorem,
		\startformula
			𝔼\brnd{∫_0^1 B_t^2 \d t}^2  \NC  =  𝔼\brnd{∫_0^1 B_t^2 \d t ∫_0^1 B_s^2 \d s}  =  𝔼\brnd{∫_0^1 ∫_0^1 B_t^2 B_s^2 \d s \d t}  =  ∫_0^1 ∫_0^1 𝔼(B_t^2 B_s^2) \d s \d t .
		\stopformula
		\startformula \startalign[n=3, align={left, right, left}]
			\NC  \text{Now, } ∀s ∈ [0, t],  \NC  𝔼(B_t^2 B_s^2)  \NC  =  𝔼(𝔼(B_t^2 B_s^2 ∣ ℱ_s))  =  𝔼(B_s^2 𝔼((B_t^2 - t) + t ∣ ℱ_s))  \NR
			\NC  \NC  \NC  =  𝔼(B_s^2 ((B_s^2 - s) + t))  =  𝔼(B_s^2 ((B_s^2 - s) + t))  \NR
			\NC  \NC  \NC  =  𝔼(B_s^4 - s B_s^2 + t B_s^2)  =  3s^2 - s^2 + ts  =  2s^2 + ts .  \NR
			\NC  \text{So }  \NC  𝔼\brnd{∫_0^1 B_t^2 \d t}^2  \NC  =  2 ∫_0^1 ∫_0^t (2s^2 + ts) \d s \d t  =  \frac{7}{9} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Find \m{𝕍\brnd{∫_0^1 t^2 B_t \d t}}.}]

		By Fubini's theorem, \m{𝔼\brnd{∫_0^1 t^2 B_t \d t}  =  ∫_0^1 t^2 𝔼B_t \d t  =  0}. So by Fubini's theorem (again),
		\startformula \startalign[align={left, left}]
			\NC  𝕍 \brnd{∫_0^1 t^2 B_t \d t}  \NC  =  𝔼 \brnd{∫_0^1 t^2 B_t \d t}^2  =  𝔼 \brnd{∫_0^1 t^2 B_t \d t ∫_0^1 s^2 B_s \d s}  \NR
			\NC  \NC  =  𝔼\brnd{∫_0^1 ∫_0^1 t^2 s^2 B_t B_s \d s \d t}  =  ∫_0^1 ∫_0^1 t^2 s^2 𝔼(B_t B_s) \d s \d t  \NR
			\NC  \NC  =  ∫_0^1 ∫_0^1 t^2 s^2 (t ∧ s) \d s \d t  =  2 ∫_0^1 ∫_0^t t^2 s^2 s \d s \d t  =  \frac{1}{14} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Calculate \m{∫_0^T e^{B_t^2} \d B_t}.}]
		Let \m{f(x) = ∫_0^x e^{t^2} \d t}. Then \m{f'(x) = e^{x^2}} and \m{f''(x) = 2 x e^{x^2}}.

		Now, using the Itô formula, we get \m{\d \brnd{∫_0^x e^{t^2} \d t} = e^{B_t^2} \d B_t + B_t e^{B_t^2} \d t}, which gives us \m{∫_0^T e^{B_t^2} \d B_t =  ∫_0^{B_T} e^{t^2} \d t - ∫_0^T B_t e^{B_t^2} \d t}.
	\stopsection

\stopchapter

\stopcomponent
