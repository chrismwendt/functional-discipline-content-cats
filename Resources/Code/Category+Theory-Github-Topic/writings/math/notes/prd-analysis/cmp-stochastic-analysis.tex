\startcomponent *

\product  prd-analysis


\startchapter [title={Brownian motion}]

	In the following, let \m{i âˆˆ [n]} for \m{n âˆˆ â„•}, and
	\startitemize [5, joinedup]
		\item  \m{Î”_i t = t_i - t_{i-1}} with \m{Î”_1 t = t_1},
		\item  \m{Î”_i x = x_i - x_{i-1}}, with  and \m{Î”_1 x = x_1},
		\item  \m{Î”_i X = X_{t_i} - X_{t_{i-1}}}, with  and \m{Î”_1 X = X_{t_1}}.
	\stopitemize

	\starttheorem
		The following are equivalent
		\startitemize [m, joineup]
			\item[bm-increment]  \m{X_t} is a stochastic process having independent Gaussian increments.
			\item[bm-marginal]  \m{X_t} is a stochastic process with marginal distributions given by
				\startformula
					Î¼_{t_1, â€¦, t_n}(A) = \frac{1}{\sqrt{(2Ï€)^n âˆ Î”_i t}} âˆ«_A \exp \brnd{-\frac12 âˆ‘ \frac{(Î”_i x)^2}{Î”_i t}} âˆ \d x_i
				\stopformula
				for any \m{0 < t_1 < â‹¯ < t_n} and \m{n âˆˆ â„•}.
			\item[bm-Fourier]  \m{X_t} is a stochastic process such that for any \m{0 < t_1 < â‹¯ < t_n} and \m{Î»_1, â‹¯, Î»_n âˆˆ â„},
				\startformula
					ğ”¼ \exp \brnd{ğš¤ âˆ‘ Î»_i Î”_i X} = \exp\brnd{-\frac12 âˆ‘ Î»_i^2 Î”_i t} .
				\stopformula
		\stopitemize
	\stoptheorem

	\startproof
		Firstly, let \m{Î  = \bcrl{(-âˆ, c_1] Ã— â‹¯ Ã— (-âˆ, c_n]: c_i âˆˆ â„, n âˆˆ â„•}}, and note that \m{Î } is a Ï€-system that generates the Borel sigma algebra. So it is enough to show the results for an arbitrary set in \m{Î }. Moreover, the idea of the proof is the same for \m{n â‰¥ 2}, so we shall consider \m{n = 2}. Therefore, \in[bm-marginal] reduces to
		\startformula
			Î¼_{t_1, t_2}((-âˆ, c_1] Ã— (-âˆ, c_2]) = \frac{1}{\sqrt{(2Ï€)^2 t_1 (t_2 - t_1)}} âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2} e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}}  \d x_2 \d x_1 ,
		\stopformula
		and \in[bm-Fourier] reduces to
		\startformula
			ğ”¼ e^{ğš¤ (Î»_1 X_{t_1} + Î»_2 (X_{t_2} - X_{t_1}))} = e^{-\frac12 (Î»_1^2 t_1 + Î»_2^2 (t_2 - t_1))} .
		\stopformula
		Now we start with the proof.
		\startitemize [i]
			\item  \in[bm-increment] âŸ¹ \in[bm-marginal]
				\startformula  \startalign[align={left, left}]
					\NC  Î¼_{t_1, t_2}((-âˆ, c_1] Ã— (-âˆ, c_2])  \NR
					\NC  =  â„™ \bcrl{X_{t_1} â‰¤ c_1, X_{t_2} â‰¤ c_2}  \NR
					\NC  =  âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2}  â„™ \bcrl{X_{t_2} âˆˆ \d x_2 âˆ£ X_{t_1} = x_1}  â„™ \bcrl{X_{t_1} âˆˆ \d x_1}  \NR
					\NC  =  âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2}  â„™ \bcrl{X_{t_2} - X_{t_1} âˆˆ \d x_2 - x_1 âˆ£ X_{t_1} = x_1}  â„™ \bcrl{X_{t_1} âˆˆ \d x_1}  \NR
					\NC  =  âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2}  â„™ \bcrl{X_{t_2} - X_{t_1} âˆˆ \d x_2 - x_1}  â„™ \bcrl{X_{t_1} âˆˆ \d x_1}  \qquad  \NC  \comment{[\text{independent incr}]}  \NR
					\NC  =  âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2}  \frac{1}{\sqrt{(2Ï€) (t_2 - t_1)}} e^{-\frac12 \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}  \d x_2  \frac{1}{\sqrt{(2Ï€) t_1}} e^{-\frac12 \frac{x_1^2}{t_1}}  \d x_1  \qquad  \NC  \comment{[\text{Gaussian incr}]}  \NR
					\NC  =  \frac{1}{\sqrt{(2Ï€)^2 t_1 (t_2 - t_1)}} âˆ«_{-âˆ}^{c_1} âˆ«_{-âˆ}^{c_2} e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}} \d x_2 \d x_1 .  \NR
				\stopalign  \stopformula

			\item  \in[bm-marginal] âŸ¹ \in[bm-Fourier]
				First, we recall the characteristic function of \m{X âˆ¼ ğ’©(0, t)} as \m{ğ”¼e^{ğš¤ Î» X} = e^{-\frac12 Î»^2 t}}.
				Secondly, note that
				\startformula
					ğ”¼ e^{ğš¤ (Î»_1 X_{t_1} + Î»_2 (X_{t_2} - X_{t_1}))}
					=  âˆ«_â„ âˆ«_â„  e^{ğš¤ (Î»_1 x_1 + Î»_2 (x_2 - x_1))}  \frac{e^{-\frac12 \brnd{\frac{x_1^2}{t_1} + \frac{\brnd{x_2 - x_1}^2}{t_2 - t_1}}}}{\sqrt{(2Ï€)^2 t_1 (t_2 - t_1)}} \d x_2 \d x_1 .
				\stopformula
				Now, using the change of variables \m{y_1 = x_1, y_2 = x_2 - x_1}, we have \m{\d y_1 = \d x_1} and \m{\d y_2 = \d x_2 - \d x_1}, so \m{\d x_2 \d x_1 = (\d y_2 + \d y_1) \d y_1 = \d y_2 \d y_1}. Therefore,
				\startformula  \startalign[align={left, left}]
					\NC  ğ”¼ e^{ğš¤ (Î»_1 X_{t_1} + Î»_2 (X_{t_2} - X_{t_1}))}  \NR
					\NC  =  âˆ«_â„ âˆ«_â„  e^{ğš¤ (Î»_1 y_1 + Î»_2 y_2)}  \frac{e^{-\frac12 \brnd{\frac{y_1^2}{t_1} + \frac{y_2^2}{t_2 - t_1}}}}{\sqrt{(2Ï€)^2 t_1 (t_2 - t_1)}}  \d y_2 \d y_1  \NR
					\NC  =  \brnd{âˆ«_â„  e^{ğš¤ Î»_1 y_1}  \frac{e^{-\frac12 \frac{y_1^2}{   t_1   }}}{\sqrt{(2Ï€)     t_1    }}  \d y_1}
					        \brnd{âˆ«_â„  e^{ğš¤ Î»_2 y_2}  \frac{e^{-\frac12 \frac{y_2^2}{t_2 - t_1}}}{\sqrt{(2Ï€) (t_2 - t_1)}}  \d y_2}  \NR
					\NC  =  \brnd{e^{-\frac12 Î»_1^2 t_1}} \brnd{e^{-\frac12 Î»_2^2 (t_2 - t_1)}}  \quad
					     =  \quad  e^{-\frac12 (Î»_1^2 t_1 + Î»_2^2 (t_2 - t_1))}  .  \NR
				\stopalign  \stopformula

			\item  \in[bm-Fourier] âŸ¹ \in[bm-increment]

		\stopitemize
	\stopproof
\stopchapter


\startchapter [title={Classification of stochastic processes}]

	This is well written in Cosma Rohilla Shalizi - Almost None of the Theory of Stochastic (2010), Chapter 1. Let \m{X} be a stochastic process given by
	\startformula \startalign[n=3, align={right,right,left}]
		\NC  X :  \NC  ğ•‹  Ã—   Î©  \NC  â†’  Î  \NR
		\NC       \NC  \qquad â„±  \NC  â†’  ğ“§  \NR
		\NC       \NC  (t,    Ï‰)  \NC  â†¦  X(t, Ï‰) .  \NR
	\stopalign \stopformula

	The spaces are as follows.

	\dscr{\m{ğ•‹}}  The \emph{index set}. Can be finite, discrete (countable) or continuous (uncountable). Can be one-sided, two-sided, spatially distributed, or sets.

	\dscr{\m{(Î, ğ“§)}}  The \emph{state space}. Requirements: measurable. Can be finite, discrete or continuous.

	\dscr{\m{(Î©, â„±, â„™)}}  The \emph{probability space}.

	\startitemize [1, nowhite, after]
		\item  If \m{ğ•‹ = \bcrl{1}, Î = â„}, then \m{X} is a \emph{random variable}.
		\item  If \m{ğ•‹ = \bcrl{1, â€¦, n}, Î = â„}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = \bcrl{1}, Î = â„^d}, then \m{X} is a \emph{random vector}.
		\item  If \m{ğ•‹ = â„•, Î = â„}, then \m{X} is a \emph{one-sided random sequence} or \emph{one-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤, Î = â„}, then \m{X} is a \emph{two-sided random variable} or \emph{two-sided discrete-time stochastic process}.
		\item  If \m{ğ•‹ = â„¤^d, Î = â„}, then \m{X} is a \emph{spatial random variable}.
		\item  If \m{ğ•‹ = â„, Î = â„}, then \m{X} is a \emph{continuous-time random variable}.
		\item  If \m{ğ•‹ = â„¬, Î = [0, âˆ]}, then \m{X} is a \emph{random set function on the reals}.
		\item  If \m{ğ•‹ = â„¬ Ã— â„•, Î = [0, âˆ]}, then \m{X} is a \emph{one-sided random sequence of set function on the reals}.
		\item  \emph{Emperical measures.}  Let \m{(Z_n)} be an i.i.d. random sequence and define \m{\hat{â„™}_n : â„¬ Ã— Î©  \NC  â†’  ğ“Ÿ: (B, Ï‰)  â†¦  \hat{â„™}_n(B, Ï‰)  =  \frac{1}{n} âˆ‘_{j = 1}^n ğŸ™_B(Z_j(Ï‰))}. Then \m{\hat{â„™}_n} is a \emph{one-sided random sequence of set function on the reals}, which are in fact \emph{probability measures}. \comment{[\m{ğ“Ÿ} is the space of probability measures on \m{â„}.]}
		\item  If \m{ğ•‹ = â„¬^d, Î = [0, âˆ]}, then \m{X} is the class of set functions on \m{â„^d}. Let \m{ğ“œ} be the subclass of measures. Then a random set function with realizations in \m{ğ“œ} is called a \emph{random measure}.
		\item  If \m{ğ•‹ = â„¬^d, \abs{Î} < âˆ}, then \m{X} is a \emph{point process}.
		\item  If \m{ğ•‹ = [0, âˆ), Î = â„^d < âˆ}. A \m{Î}-valued random process on \m{ğ•‹} with paths in \m{C(ğ•‹)} is a \emph{continuous random process}. E.g. Wiener process.
	\stopitemize

\stopchapter


\startchapter [title={Martingales}, reference=sub:martingales]

	\startsection [title={New martingales from old}]

		A stochastic process \m{A = (A_n)} is called adapted if \m{âˆ€ n âˆˆ â„•, A_n âˆˆ L^0(â„±_n)}. Let \m{M = (M_n)} be a martingale. Then process \m{\tilde{M} = (\tilde{M}_n)} defined by \m{(A â‹… M)_n = \tilde{M}_n = âˆ‘_{j = 0}^{n - 1} A_j Î”M_j}, where \m{Î”M_j = M_{j + 1} - M_j}, is called the \emph{martingale transform} of \m{M} by \m{A}.

		\starttheorem [title={martingale transform theorem}]
			\m{\tilde{M}} is a martingale.
		\stoptheorem
		\startproof
			\m{ğ”¼(Î” \tilde{M}_n âˆ£ â„±_n)  =  ğ”¼(A_n Î”M_n âˆ£ â„±_n)  =  A_n ğ”¼(Î”M_n âˆ£ â„±_n)  =  0}.
		\stopproof

		Now, let \m{X_n} be a stochastic process and \m{Ï„} be a stopping time. Define the stopped process \m{X_Ï„ = âˆ‘_{j = 0}^âˆ ğŸ™_{\bcrl{Ï„ = j}} X_j} when \m{â„™(Ï„ < âˆ) = 1}.

		\starttheorem [title={stopping time theorem}]
			Let \m{(M_n)} be a martingale with respect to \m{(â„±_n)}. Then \m{(M_{n âˆ§ Ï„})} is also a martingale with respect to \m{(â„±_n)}.
		\stoptheorem
		\startproof
			Without loss of generality, assume \m{M_0 = 0}, otherwise we can translate by \m{M_0} as \m{\tilde{M_n} = M_n - M_0}. Now, the \emph{stake process} \m{A_n = ğŸ™_{\bcrl{Ï„ > n}} = 1 - ğŸ™_{\bcrl{Ï„ â©½ n}}} is adapted to \m{(â„±_n)} and is bounded by \m{n}. Now,
			\startformula \startalign
				\NC  (A â‹… M)_n  \NC =  âˆ‘_{j = 0}^{n - 1} A_j Î”M_j  \NR
				\NC  \NC =  âˆ‘_{j = 0}^{n - 1} Î”M_j - âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl{Ï„ â©½ j}} \brnd{M_{j + 1} - M_j}  \NR
				\NC  \NC =  M_n - M_0 - M_n ğŸ™_{\bcrl{Ï„ â©½ n}} + âˆ‘_{j = 0}^{n - 1} \brnd{ğŸ™_{\bcrl{Ï„ â©½ j}} M_j - ğŸ™_{\bcrl{Ï„ â©½ {j - 1}}} M_j}  \NR
				\NC  \NC =  M_n ğŸ™_{\bcrl{Ï„ > n}} + âˆ‘_{j = 0}^{n - 1} M_j ğŸ™_{\bcrl{Ï„ = j}}   \NR
				\NC  \NC =  M_n ğŸ™_{\bcrl{Ï„ > n}} + âˆ‘_{j = 0}^{n - 1} M_Ï„ ğŸ™_{\bcrl{Ï„ = j}}   \NR
				\NC  \NC =  M_n ğŸ™_{\bcrl{Ï„ > n}} + M_Ï„ âˆ‘_{j = 0}^{n - 1} ğŸ™_{\bcrl{Ï„ = j}}   \NR
				\NC  \NC =  M_n ğŸ™_{\bcrl{Ï„ > n}} + M_Ï„ ğŸ™_{\bcrl{Ï„ â©½ n}}   \NR
				\NC  \NC =  M_{n âˆ§ Ï„} .  \NR
			\stopalign \stopformula
			Therefore, \m{(M_{n âˆ§ Ï„})} is a martingale transform of \m{(M_n)}. Since \m{(A_n)} is bounded and adapted, by the martingale transform theorem, \m{(M_{n âˆ§ Ï„})} is a martingale.
		\stopproof

	\stopsection

\stopchapter


\startchapter [title={Markov chains}]

	\startsection [title={First Step Analysis}]
		First-step analysis is a general strategy for solving many Markov chain problems by conditioning on the first step of the Markov chain.

		We understand this from the first example of \cite[Steele2001]. We will derive a recursive relationship of the probability of a gambler winning before he goes bankrupt. The setting is as follows.

		A gambler starts with a principal of \m{0}, and he can borrow a maximum of \m{b}. He stops playing if his net value is \m{a} at any point of time. At each instant \m{i}, his wealth \m{S_i} either increases or decreases by one amount depending on the output of the Bernoulli random variable \m{X_i} with \quote{up} probability \m{p}. This gives rise to the finite state space \m{ğ’® = \bcrl{-b, -b + 1, â€¦, a - 1, a}}. Note that \m{(S_n)} is a time-homogeneous Markov chain on \m{ğ’®} with transition probabilities as follows:
		\startitemize [n, joinedup]
			\item  \m{P_{-b, j} = P_{a, j} = 0} (absorbing barriers),
			\item  \m{P_{i, i + 1} = p} and \m{P_{i, i - 1} = q} with \m{q = 1 - p}, and
			\item  \m{P_{i, j} = 0} in all other cases.
		\stopitemize

		Let \m{Ï„} be the first exit time, and \m{f(k) = â„™\bcrl{S_Ï„ = A âˆ£ S_0 = k} = â„™_{\bcrl{S_0 = k}} \bcrl{S_Ï„ = A}} for \m{k âˆˆ ğ’®}. Our goal in this setting is to obtain a recursive relation for \m{f}. Now,
		\startformula
			\bcrl{S_Ï„ = A}  =  \bcrl{S_Ï„ = A} âˆ© Î©  =  \bcrl{S_Ï„ = A} âˆ© â¨†_{l âˆˆ ğ’®} \bcrl{S_1 = l}  =  â¨†_{l âˆˆ ğ’®} \bcrl{S_Ï„ = A} âˆ© \bcrl{S_1 = l} .
		\stopformula
		Finally,
		\startformula \startalign[n=3]
			\NC  f(k)  \NC =  â„™_{\bcrl{S_0 = k}} \bcrl{S_Ï„ = A}  \NR
			\NC  \NC =  âˆ‘_{l âˆˆ ğ’®} â„™_{\bcrl{S_0 = k}} \brnd{\bcrl{S_Ï„ = A} âˆ© \bcrl{S_1 = l}}  \NR
			\NC  \NC =  âˆ‘_{l âˆˆ ğ’®} â„™_{\bcrl{S_0 = k}} \bcrl{S_Ï„ = A âˆ£ S_1 = l} â„™_{\bcrl{S_0 = k}} \bcrl{S_1 = l}  \NR
			\NC  \NC =  âˆ‘_{l âˆˆ ğ’®} â„™_{\bcrl{S_1 = l}} \bcrl{S_Ï„ = A} P_{k, l}  \qquad  \NC  [\text{Markov property}]  \NR
			\NC  \NC =  âˆ‘_{l âˆˆ ğ’®} P_{k, l} f(l)  .  \NC  [\text{time-homogenity}]  \NR
		\stopalign \stopformula

		Since \m{P_{k, l} = 0} for all \m{l} except for \m{k Â± 1}, we get
		\startformula
			f(k) = f(k + 1) p + f(k - 1) q .
		\stopformula

	\stopsection
\stopchapter


\startchapter[title={Markov processes}, reference=sub:Markov]

	\startsubject[title={Equivalent definitions}]
		Let \m{s âˆˆ [0, t]}. Then \m{X_â‹…} is a Markov process if any of the following are true:
		\startitemize[1, nowhite, after]
			\item  \m{âˆ€ E âˆˆ â„±, â„™(X_t âˆˆ E âˆ£ â„±_s) = â„™(X_t âˆˆ E âˆ£ X_s)}, or
			\item  \m{âˆ€ E âˆˆ â„±, âˆ€ f âˆˆ L^0 âˆ© â„¬, ğ”¼(f(X_t) âˆ£ â„±_s) = ğ”¼(f(X_t) âˆ£ X_s)}.
		\stopitemize
	\stopsubject

	\startsubject[title={Martingale vs Markov}]
		See \goto{djalil.chafai.net}[url(http://djalil.chafai.net/blog/2012/01/20/martingales-which-are-not-markov-chains/)] and \goto{MathSx:763645}[url(https://math.stackexchange.com/questions/763645)].
	\stopsubject
\stopchapter


\startchapter [title={ItÃ´ calculus}, reference=sub:ItÃ´]

	\startremark [title={Wiener integral}]
		The term \emph{Wiener integral} may be used to refer to either of the following two unrelated concepts
		\startitemize [i, joinedup]
			\item  Lebesgue integral w.r.t. the Wiener measure
			\item  Stochastic integral when the integrand is deterministic
		\stopitemize
	\stopremark


	\startremark [title={Notations}]
		In what follows, \m{T = [0, âˆ)}, \m{ğ’œ} means adapted, \m{â„¬} means bounded, \m{ğ’} means continuous, and \m{\norm{â‹…}} denotes the \m{L^2}-norm.

		Let \m{(Î©, â„±, ğ”½ = (â„±_t), â„™)} be a filtered probability space, \m{W:T Ã— Î© â†’ â„‚} be a \m{ğ”½}-adapted Wiener martingale, and \m{X: T Ã— Î© â†’ â„‚} be a stochastic process.
	\stopremark


	\startsection [title={Definition of the ItÃ´ integral}]

		\startsubsection [title={Step 1: \m{X âˆˆ ğ’œ âˆ© ğ’®} a.s.}]

			Let \m{X(t, Ï‰) = âˆ‘_{j â‰¥ 0} Î¾_j(Ï‰) ğŸ™_{[t_j, t_{j+1})}(t)}, where \m{Î¾_j âˆˆ L^0(â„±_{t_j})}.

		\stopsubsection

		\startsubsection [title={Step 2: \m{X âˆˆ ğ’œ âˆ© â„¬ âˆ© ğ’} a.s.}]

			Define \m{X_n(t, Ï‰) = X\brnd{\frac{\floor{nt}}{n}, Ï‰}, \ n âˆˆ â„•}. Note that \m{âˆ€n, X_n âˆˆ ğ’œ âˆ© ğ’®}, and since \m{X âˆˆ ğ’}, \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)} â†’ 0} (pointwise convergence) \m{(t, Ï‰)}-a.s. Then \m{âˆ€ Îµ > 0}, there exists a sufficiently large \m{n âˆˆ â„•} such that \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)} < Îµ < âˆ} (bounded) \m{(t, Ï‰)}-a.s, so \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)}^2 < Îµ^2 < âˆ} \m{(t, Ï‰)}-a.s. Therefore, by the bounded convergence theorem, \m{\norm{X_n - X} â†’ 0}.

			Therefore, \m{(X_n)} is Cauchy in \m{L^2(T Ã— Î©)}, that is, \m{\norm{X_n - X_m} â†’ 0}. Now, by linearity and ItÃ´ isometry for the ItÃ´ integral for simple processes, \m{\norm{â„(X_n) - â„(X_m)} = \norm{â„(X_n - X_m)} = \norm{X_n - X_m} â†’ 0}. Therefore, for \m{t âˆˆ T} fixed, \m{(â„(X_n))} is Cauchy in \m{L^2(Î©)}. Since \m{L^2(Î©)} is complete, the sequence converges. Denote the limit by \m{â„(X)}, that is, \m{\norm{â„(X_n) - â„(X)} â†’ 0}.

		\stopsubsection

		\startsubsection [title={Step 3: \m{X âˆˆ ğ’œ âˆ© â„¬ âˆ© L^0(T Ã— Î©)}}]

		\stopsubsection

		\startsubsection [title={Step 4: \m{X âˆˆ ğ’œ âˆ© L^2(T Ã— Î©)}}]

		\stopsubsection

		\startsubsection [title={Step 5: \m{X âˆˆ ğ’œ âˆ© \bcrl{X âˆˆ â„‚^{T Ã— Î©} : âˆ€ t â‰¥ 0, âˆ«_0^t X(s, â‹…) \d s < âˆ} a.s.}}]

		\stopsubsection
	\stopsection


	\startsection [title={Properties of the ItÃ´ integral}]

		In what follows, assume the following. Let \m{X, Y âˆˆ ğ’œ âˆ© L^2(T Ã— Î©); (X_n), (Y_n) âŠ‚ ğ’œ âˆ© ğ’®} such that \m{\norm{X_n - X} â†’ 0} and \m{\norm{Y_n - Y} â†’ 0}. Let \m{z âˆˆ â„‚}.

		\startsubsection [title={Linearity: \m{\norm{z â„(X) + â„(Y) - â„(zX+Y)} = 0}}]
			First, note that \m{\norm{(z X_n + Y_n) - (z X + Y)} â‰¤ \abs{z} \norm{X_n - X} + \norm{Y_n - Y} â†’ 0}. Now, by the linearity of the integral \m{â„:ğ’œ âˆ© ğ’® â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left}]
				\NC  \norm{z â„(X) + â„(Y) - â„(z X + Y)}  \NR
				\NC  =  \norm{z â„(X) + â„(Y) - z â„(X_n) - â„(Y_n) + â„(z X_n + Y_n) - â„(z X + Y)}  \NR
				\NC  â‰¤  \abs{z} \norm{â„(X) - â„(X_n)} + \norm{â„(Y) - â„(Y_n)} + \norm{â„(z X_n + Y_n) - â„(z X + Y)}  â†’  0 .
			\stopalign \stopformula
		\stopsubsection

		\startsubsection [title={ItÃ´ isometry: \m{\norm{â„(X)} = \norm{X}}}]
			Using the isometry of the integral \m{â„:ğ’œ âˆ© ğ’® â†’ L^2(Î©)}, we have
			\startformula \startalign[align={left,left}]
				\NC  \norm{â„(X)}  \NC  â‰¤  \norm{â„(X) - â„(X_n)} + \norm{â„(X_n)}  \NR
				\NC  \NC  =  \norm{â„(X) - â„(X_n)} + \norm{X_n}  \NR
				\NC  \NC  =  \norm{â„(X) - â„(X_n)} + \norm{X_n - X} + \norm{X}  â†’  \norm{X} .
			\stopalign \stopformula
			Note that the \quote{ItÃ´ isometry} is actually a unitary transformation.
		\stopsubsection

		\startsubsection [title={Martingale property: \m{ğ”¼\brnd{â„_t(X) âˆ£ â„±_s} = â„_s(X)} a.s.}]
			The martingale property of the integral \m{â„:ğ’œ âˆ© ğ’® â†’ L^2(Î©)} gives \m{ğ”¼\brnd{â„_t(X_n) - â„_s(X_n) âˆ£ â„±_s} = 0}. Using this and the unitariness of the ItÃ´ isometry, we get
			\startformula \startalign[align={left,left}]
				\NC  \norm{ğ”¼{\brnd{â„_t(X) - â„_s(X) âˆ£ â„±_s}}}^2  \NR
				\NC  =  ğ”¼\abs{ğ”¼{\brnd{â„_t(X) - â„_t(X_n) + â„_s(X_n) - â„_s(X) âˆ£ â„±_s}} + ğ”¼{\brnd{â„_t(X_n) - â„_s(X_n) âˆ£ â„±_s}}}^2  \NR
				\NC  =  ğ”¼\abs{ğ”¼{\brnd{â„_t(X) - â„_t(X_n) + â„_s(X_n) - â„_s(X) âˆ£ â„±_s}} + 0}^2  \NR
				\NC  â‰¤  ğ”¼ğ”¼\brnd{\abs{â„_t(X) - â„_t(X_n) + â„_s(X_n) - â„_s(X)}^2 âˆ£ â„±_s}  \NR
				\NC  =  ğ”¼\abs{â„_t(X) - â„_t(X_n) + â„_s(X_n) - â„_s(X)}^2  \NR
				\NC  =  \norm{â„_t(X - X_n) + â„_s(X_n - X)}^2  \NR
				\NC  â‰¤  2 \brnd{\norm{â„_t(X - X_n)}^2 + \norm{â„_s(X_n - X)}^2}  \qquad  \bsqr{\norm{a + b}^2 â‰¤ 2 \brnd{\norm{a}^2 + \norm{b}^2}}  \NR
				\NC  =  2 \brnd{\norm{X - X_n}^2 + \norm{X_n - X}^2}  =  4 \norm{X_n - X}^2  â†’  0 .
			\stopalign \stopformula
		\stopsubsection

	\stopsection

	% In what follows, \m{T = [0, t], \ t âˆˆ [0, âˆ)}, \m{X: T Ã— Î© â†’ â„}, and \m{ğ’®} represent the class of simple processes.

	% \startsection [title={Approximation of \m{ğ’® âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]
	% 	\startsubsection [title={Step 1: \m{ğ’® âˆ‹ X_n â†’ X âˆˆ C âˆ© B}}]
	% 		Suppose \m{X} is a progressive, adapted, \emph{bounded on \m{I}} \m{t}-a.s., and has \emph{continuous sample paths} \m{t}-a.s. Then there exist a sequence of bounded simple processes \m{(X_n)} such that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Proof.

	% 		Let \m{(Î”_n)} be a sequence of progressively finer partitions of \m{I} such that \m{Î”_n = \bcrl{t_0, t_1 â€¦, t_n]; t_0 < t_1 < â€¦ < t_n}, and \m{\norm{Î”_n] â†’ 0} as \m{n â†’ âˆ}. Define the sequence of simple processes \m{X_n(t, Ï‰) = âˆ‘_{j = 0}^{n - 1} X(t_j, Ï‰) ğŸ™_{\intco[t_j, t_{j + 1}]}(t)}. Then, we claim that \m{X_n â†’ X} in \m{L^2(T Ã— Î©)}.

	% 		Firstly fix \m{Ï‰ âˆˆ Î©}. By the \emph{continuity of paths} of \m{X}, \m{âˆ€ Ïµ > 0, âˆƒ Î´ > 0} such that whenever \m{\abs{s - t] < Î´}, we have \m{\abs{X(s, Ï‰) - X(t, Ï‰)] < Ïµ}. Therefore, for \m{\norm{Î”_n] < Î´}, we have
	% 		\startformula
	% 			\abs{X_n(t, Ï‰) - X(t, Ï‰)]
	% 				â‰¤ âˆ‘_{j = 0}^{n - 1} \abs{X(t_j, Ï‰) - X(t, Ï‰)] ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				< âˆ‘_{j = 0}^{n - 1} Ïµ ğŸ™_{\intco[t_j, t_{j + 1}]}(t)
	% 				= Ïµ .
	% 		\stopformula
	% 		Therefore, \m{âˆ€ t âˆˆ I}, \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)] â†’ 0}, and so \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)]^2 â†’ 0}. Moreover, since the sample path \m{X(â‹…, Ï‰)} is \emph{bounded}, we have \m{\abs{X_n(t, Ï‰) - X(t, Ï‰)]^2 â©½ 4 \norm{X(â‹…, Ï‰)]_{L^âˆ(I)}^2 < âˆ}. Therefore by the bounded convergence theorem, \m{âˆ«_0^1 \abs{X_n(t, Ï‰) - X(t, Ï‰)]^2 \d t â†’ 0}.

	% 		Now varying \m{Ï‰} and taking expectation, we get \m{ğ”¼\brnd{âˆ«_0^1 {\abs{X_n(t, Ï‰) - X(t, Ï‰)]}^2 \d t] â†’ 0}. In other words, \m{\norm{X_n - X]_{L^2(T Ã— Î©)} â†’ 0}.
	% 	\stopsubsection
	% 	\startsubsection [title={Step 2: \m{C âˆ© B âˆ‹ X_n â†’ X âˆˆ B}}]

	% 	\stopsubsection
	% 	\startsubsection [title={Step 3: \m{B âˆ‹ X_n â†’ X âˆˆ L^2(T Ã— Î©)}}]

	% 	\stopsubsection
	% \stopsection

	\startsection[title={ItÃ´ formula for multidimensional processes}]

		Recall the discussion of Taylor series for multivariate functionals in section \in[sec:taylor].

		This part is from \cite[SundarKallianpur2014], Â§ 5.3, 5.4 and 5.6.

		A (local, continuous) semimartingale is a process \m{X_t} that can be written as \m{X_t = X_0 + M_t + A_t}, where
		\startitemize [n, joinedup, nowhite, after]
			\item  \m{M_t} is a mean-zero (local, continuous) martingale, and
			\item  \m{A_t} is an right-continuous adapted process of locally bounded variation.
		\stopitemize
		This is equivalently represented in the differential form as \m{\d X_t = \d M_t + \d A_t}.

		Let \m{X_t} be a \m{d}-dimensional semimartingale, and let \m{Y_t = f(X_t)}, where \m{f âˆˆ C^2(â„)}. Then
		\startformula
			\d Y_t
			=  \d f(X_t)
			=  f'(X_t) \d A_t
			 + f'(X_t) \d M_t
			 + \frac12 f''(X_t) \d \inn{M_t, M_t},
		\stopformula
		where we use the rule \m{\d \inn{B^{(j)}, B^{(j)}}_t = \brnd{\d B_t^{(j)}}^2 = \d t}, everything else being 0.

		Alternatively, as from \cite[Kuo2006], we have the following.

		Let \m{X_t= (X_t^{(1)}, \dots, X_t^{(d)})} be a \m{d}-dimensional process and \m{f(t, x)} be a functional of \m{(t, X_t)}. Then the ItÃ´ formula becomes
		\startformula
			\d f(t, X_t)  =  \brnd{\inn{\d t, \D_t} f}(t, X_t)
				+ \brnd{\inn{\d x, \D_x} f}(t, X_t)
				+ \frac12 \brnd{\inn{\d x, \D_x}^2 f}(t, X_t) ,
		\stopformula
		or in short, \m{\d f  =  \brnd{\inn{\d t, \D_t}  +  \inn{\d x, \D_x}  +  \frac12 \inn{\d x, \D_x}^2} f}.

	\stopsection

	\startsection[title={Adapted and instantly independent implies deterministic}]

		Let \m{X_â‹…} be both adapted and instantly independent. Then for any fixed \m{t}, we have \m{X_t = ğ”¼(X_t âˆ£ â„±_t) = ğ”¼ X_t}. Therefore, \m{X_t} is constant w.r.t. \m{Ï‰} for each \m{t}. Therefore \m{X_â‹…} must be deterministic.

	\stopsection

\stopchapter


\startchapter [title={Examples}]

	\startsection [title={Find \m{ğ”¼\brnd{âˆ«_0^1 B_t^2 \d t}^2}.}]

		By Fubini's theorem,
		\startformula
			ğ”¼\brnd{âˆ«_0^1 B_t^2 \d t}^2  \NC  =  ğ”¼\brnd{âˆ«_0^1 B_t^2 \d t âˆ«_0^1 B_s^2 \d s}  =  ğ”¼\brnd{âˆ«_0^1 âˆ«_0^1 B_t^2 B_s^2 \d s \d t}  =  âˆ«_0^1 âˆ«_0^1 ğ”¼(B_t^2 B_s^2) \d s \d t .
		\stopformula
		\startformula \startalign[n=3, align={left, right, left}]
			\NC  \text{Now, } âˆ€s âˆˆ [0, t],  \NC  ğ”¼(B_t^2 B_s^2)  \NC  =  ğ”¼(ğ”¼(B_t^2 B_s^2 âˆ£ â„±_s))  =  ğ”¼(B_s^2 ğ”¼((B_t^2 - t) + t âˆ£ â„±_s))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  =  ğ”¼(B_s^2 ((B_s^2 - s) + t))  \NR
			\NC  \NC  \NC  =  ğ”¼(B_s^4 - s B_s^2 + t B_s^2)  =  3s^2 - s^2 + ts  =  2s^2 + ts .  \NR
			\NC  \text{So }  \NC  ğ”¼\brnd{âˆ«_0^1 B_t^2 \d t}^2  \NC  =  2 âˆ«_0^1 âˆ«_0^t (2s^2 + ts) \d s \d t  =  \frac{7}{9} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Find \m{ğ•\brnd{âˆ«_0^1 t^2 B_t \d t}}.}]

		By Fubini's theorem, \m{ğ”¼\brnd{âˆ«_0^1 t^2 B_t \d t}  =  âˆ«_0^1 t^2 ğ”¼B_t \d t  =  0}. So by Fubini's theorem (again),
		\startformula \startalign[align={left, left}]
			\NC  ğ• \brnd{âˆ«_0^1 t^2 B_t \d t}  \NC  =  ğ”¼ \brnd{âˆ«_0^1 t^2 B_t \d t}^2  =  ğ”¼ \brnd{âˆ«_0^1 t^2 B_t \d t âˆ«_0^1 s^2 B_s \d s}  \NR
			\NC  \NC  =  ğ”¼\brnd{âˆ«_0^1 âˆ«_0^1 t^2 s^2 B_t B_s \d s \d t}  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 ğ”¼(B_t B_s) \d s \d t  \NR
			\NC  \NC  =  âˆ«_0^1 âˆ«_0^1 t^2 s^2 (t âˆ§ s) \d s \d t  =  2 âˆ«_0^1 âˆ«_0^t t^2 s^2 s \d s \d t  =  \frac{1}{14} .  \NR
		\stopalign \stopformula

	\stopsection

	\startsection [title={Calculate \m{âˆ«_0^T e^{B_t^2} \d B_t}.}]
		Let \m{f(x) = âˆ«_0^x e^{t^2} \d t}. Then \m{f'(x) = e^{x^2}} and \m{f''(x) = 2 x e^{x^2}}.

		Now, using the ItÃ´ formula, we get \m{\d \brnd{âˆ«_0^x e^{t^2} \d t} = e^{B_t^2} \d B_t + B_t e^{B_t^2} \d t}, which gives us \m{âˆ«_0^T e^{B_t^2} \d B_t =  âˆ«_0^{B_T} e^{t^2} \d t - âˆ«_0^T B_t e^{B_t^2} \d t}.
	\stopsection

\stopchapter

\stopcomponent
